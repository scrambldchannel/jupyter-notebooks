{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**: This is a work in progress\n",
    "\n",
    "# Exploring the trees of Berlin's streets\n",
    "\n",
    "The folk at [Code for Berlin](https://www.codefor.de/berlin/) have created a REST API offering access to the database of Berlin street trees and have [an issue open](https://github.com/codeforberlin/tickets/issues/3) asking people to try to do \"something\" with it. It seemed a cool way to look more deeply into the architecture of REST APIs on both the client and server side as well as playing with an interesting dataset, given I live in Berlin and like trees.\n",
    "\n",
    "The API itself is built using the [Django REST Framework](https://www.django-rest-framework.org/) and is hosted [here](https://github.com/codeforberlin/trees-api-v2). An [interactive map](https://trees.codefor.de/) exists which uses the api to plot all the trees and allows some simple filtering on top of tiles from Open Street Map. I took a look and it proved a great intro to the data I wanted to do a deeper analysis of the data.\n",
    "\n",
    "Some of the things I wanted to look into were:\n",
    "\n",
    "* Which areas have the most trees, the oldest trees etc\n",
    "* Are there any connections between the number of trees and other datapoints (air quality, socioeconomic demographics etc)\n",
    "* Why are there no trees showing on my street even though I can see some out the window as I type this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What sort of data is there and how can it be consumed? \n",
    "\n",
    "One of the cool things about the Django REST Framework is the way it's API can be explored out of the box. Simply point your browser to the API using the following link:\n",
    "\n",
    "https://trees.codefor.de/api/v2\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "```json\n",
    "\n",
    "HTTP 200 OK\n",
    "Allow: GET, HEAD, OPTIONS\n",
    "Content-Type: application/json\n",
    "Vary: Accept\n",
    "\n",
    "{\n",
    "    \"trees\": \"https://trees.codefor.de/api/v2/trees/\",\n",
    "    \"species\": \"https://trees.codefor.de/api/v2/species/\",\n",
    "    \"genera\": \"https://trees.codefor.de/api/v2/genera/\",\n",
    "    \"boroughs\": \"https://trees.codefor.de/api/v2/boroughs/\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Essetially this is telling us that we have four endpoints - trees, species, genera and boroughs. You can follow the links to each one to get more details. To explore the data available, I hacked together a simple python wrapper which you can find here: \n",
    "\n",
    "https://github.com/scrambldchannel/berlin-trees-api-pywrapper\n",
    "\n",
    "### Usage\n",
    "\n",
    "The wrapper can be installed via pip:\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/scrambldchannel/berlin-trees-api-pywrapper.git\n",
    "```\n",
    "\n",
    "#### Setup the wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module and other useful libs\n",
    "\n",
    "import json\n",
    "from berlintreesapiwrapper import TreesWrapper\n",
    "\n",
    "# Instantiate the api wrapper object\n",
    "# you can change the base url if you are running a local instance of the api \n",
    "\n",
    "base_url = \"https://trees.codefor.de/api/\"\n",
    "api_version = 2\n",
    "\n",
    "api = TreesWrapper(api_root = base_url, version = api_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling functions\n",
    "\n",
    "There is a function defined for each endpoint. At this stage, each function accepts only a couple of parameters. Each endpoint returns paginated results (the current config seems to return ten results per page) so the page number is a valid parameter for each function, defaulting to 1 if not supplied. See examples below.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees endpoint\n",
    "\n",
    "The most versatile endpoint is the trees endpoint which returns sets of individual trees. The endpoint allows filtering in a number of different ways (see https://github.com/codeforberlin/trees-api-v2#making-queries).\n",
    "\n",
    "My basic wrapper function doesn't support anything other than a simple dump of all trees, by page, at this stage. This was sufficient for pulling all the data but I will look into enhancing this wrapper later, the ability to filter trees based on location is particular interesting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eg. request first page of all trees\n",
    "\n",
    "ret_trees = api.get_trees()\n",
    "print(json.dumps(ret_trees.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eg. request the 5000th page of all trees\n",
    "\n",
    "ret_trees = api.get_trees(page=5000)\n",
    "print(json.dumps(ret_trees.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other endpoints\n",
    "\n",
    "The other endpoints just return a count of the trees by borough, species and genus. Results can be filtered by page and the name of the borough etc. See examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eg. request first page of the borough count\n",
    "\n",
    "ret_borough = api.get_boroughs()\n",
    "print(json.dumps(ret_borough.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eg. request the count for a specific borough\n",
    "\n",
    "ret_borough = api.get_boroughs(borough = \"Friedrichshain-Kreuzberg\")\n",
    "print(json.dumps(ret_borough.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eg. request the count for a specific species\n",
    "\n",
    "ret_species = api.get_species(species = \"Fagus sylvatica\")\n",
    "print(json.dumps(ret_species.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eg. request a specific page of the count of genera\n",
    "\n",
    "ret_genera = api.get_genera(page = 13)\n",
    "print(json.dumps(ret_genera.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration \n",
    "\n",
    "Now we have a framework for pulling the data, let's create some simple visualisations to give an overview of the data. \n",
    "\n",
    "### Visualising the number of trees per borough\n",
    "\n",
    "This helps give an overview of the data (and playing with graphs is cool)\n",
    "\n",
    "#### Pull data into a dataframe and create simple bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all pages, create a dataframe and create simple horizontal bar graph\n",
    "\n",
    "page = 1\n",
    "boroughs = []\n",
    "counts = []\n",
    "df =  pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "\n",
    "while(True):\n",
    "    result = api.get_boroughs(page = page).json()\n",
    "    for i in result['results']:\n",
    "        boroughs.append(i['borough'])\n",
    "        counts.append(i['count'])\n",
    "    if(result['next'] is None):\n",
    "        break\n",
    "    page = page + 1\n",
    "    \n",
    "df = pd.DataFrame({'borough': boroughs, 'count' : counts} )\n",
    "\n",
    "ax = df.plot.barh(figsize=(30, 14) , x = 'borough', color='green', alpha = 0.3, width = 0.9, title = \"Number of trees by Borough\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot these figures onto a simple map\n",
    "\n",
    "To get a simple shapefile of the administrative boundaries of Berlin I used this interface which makes it easy to select the areas you want and export. There are other ways of doing this though, such as using the Overpass API, which I will definitely look into in the future. \n",
    "\n",
    "https://wambachers-osm.website/boundaries/\n",
    "\n",
    "I've saved the shapefiles into a local directory and use this code to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a couple of necessary libraries\n",
    "\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "\n",
    "# set path for data imports\n",
    "\n",
    "dataset_path = \"../datasets/\"\n",
    "\n",
    "# Import the file into a geodataframe\n",
    "\n",
    "gdf = gpd.read_file(dataset_path + 'Berlin_AL9-AL9.shp', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to combine the dataframe containing the tree counts by borough with the data from the shapefile. Happily, I know the borough names correspond to the names in the shapefile from osm so we can join them together like this using the merge method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge counts from dataframe with our geodataframe into new geodataframe\n",
    "\n",
    "map_gdf = gdf.merge(df, left_on='name', right_on='borough')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a \"chloropleth\" as per howto here - https://towardsdatascience.com/lets-make-a-map-using-geopandas-pandas-and-matplotlib-to-make-a-chloropleth-map-dddc31c1983d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a variable that will call whatever column we want to visualise on the map\n",
    "\n",
    "variable = \"count\"\n",
    "\n",
    "# set the range , figures and axes\n",
    "\n",
    "vmin, vmax = 500, 60000\n",
    "fig, ax = plt.subplots(1, figsize=(30, 14))\n",
    "\n",
    "# create map\n",
    "map_gdf.plot(column=variable, cmap='Greens',  linewidth=1.5, ax=ax, edgecolor='0.5')\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# add a title\n",
    "ax.set_title(\"Number of street trees by Borough\", horizontalalignment=\"center\",  fontdict={\"fontsize\": \"30\", \"fontweight\" : \"5\"})\n",
    "# create an annotation for the data source\n",
    "ax.annotate(\"Source: Code for Berlin\",xy=(0.1, .08), xycoords=\"figure fraction\", horizontalalignment=\"left\", verticalalignment=\"top\", fontsize=12, color=\"#555555\")\n",
    "\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap=\"Greens\", norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "# add the colorbar to the figure\n",
    "cbar = fig.colorbar(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull all the data from the trees endpoint to do more detailed analysis\n",
    "\n",
    "Looking at counts of trees is fine but to really analyse the data, I want to pull it all individual trees into a single dataframe. To do so, I returned to the trees endpoint. The relevant part of the json result is contained within \"features\" and an individual tree looks like this:\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "    \"geometry\": {\n",
    "        \"coordinates\": [\n",
    "            13.357809221770479,\n",
    "            52.56657685261005\n",
    "        ],\n",
    "        \"type\": \"Point\"\n",
    "    },\n",
    "    \"id\": 38140,\n",
    "    \"properties\": {\n",
    "        \"age\": 80,\n",
    "        \"borough\": \"Reinickendorf\",\n",
    "        \"circumference\": 251,\n",
    "        \"created\": \"2018-11-11T12:22:35.506000Z\",\n",
    "        \"feature_name\": \"s_wfs_baumbestand_an\",\n",
    "        \"genus\": \"ACER\",\n",
    "        \"height\": 20,\n",
    "        \"identifier\": \"s_wfs_baumbestand_an.7329\",\n",
    "        \"species\": \"Acer pseudoplatanus\",\n",
    "        \"updated\": \"2018-11-11T12:22:35.506000Z\",\n",
    "        \"year\": 1938\n",
    "    },\n",
    "    \"type\": \"Feature\"\n",
    "},\n",
    "```\n",
    "\n",
    "Essentially I want to pull all of these trees into a single dataframe by iterating over every page of the trees endpoint. I hacked together this code to accomplish this. It also converted the result to a geodataframe based on the long/lat information returned. Note, this was really slow, probably wasn't the best way to do it and there are other ways of sourcing the raw data. That said, I wanted to do it as a PoC.\n",
    "\n",
    "```python\n",
    "\n",
    "while True:\n",
    "    this_page = api.get_trees(page=page).json()\n",
    "    next_page = this_page[\"next\"]\n",
    "    for row in range(len(this_page['features'])):\n",
    "        ids.append(this_page['features'][row]['id'])\n",
    "        age.append(this_page['features'][row]['properties']['age'])\n",
    "        borough.append(this_page['features'][row]['properties']['borough'])\n",
    "        circumference.append(this_page['features'][row]['properties']['circumference'])\n",
    "        genus.append(this_page['features'][row]['properties']['genus'])\n",
    "        height.append(this_page['features'][row]['properties']['height'])\n",
    "        species.append(this_page['features'][row]['properties']['species'])\n",
    "        year.append(this_page['features'][row]['properties']['year'])        \n",
    "        lat.append(this_page['features'][row]['geometry']['coordinates'][0])\n",
    "        long.append(this_page['features'][row]['geometry']['coordinates'][1])        \n",
    "\n",
    "    page = page + 1\n",
    "\n",
    "    # for debugging, can be removed at some point\n",
    "    print(page)\n",
    "\n",
    "    if(next_page) is None:\n",
    "        break\n",
    "\n",
    "# create dataframe from resulting arrays       \n",
    "df = pd.DataFrame(\n",
    "    {'id': ids,\n",
    "    'age' : age,\n",
    "    'borough' : borough,\n",
    "    'circumference' : circumference,\n",
    "    'genus' : genus,\n",
    "    'height' : height,\n",
    "    'species' : species,\n",
    "    'year': year,\n",
    "    'Latitude': lat,\n",
    "    'Longitude': long})\n",
    "\n",
    "# convert to geodataframe\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude)) \n",
    "\n",
    "```\n",
    "\n",
    "Happily, I saved it to a csv for future analysis. On that note, I generally find it much easier to do this sort of exploration of data, once it text form, using the amazing [VisiData](https://visidata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv into geodataframe\n",
    "\n",
    "all_trees_gdf = gpd.read_file(dataset_path + 'all_trees_gdf.csv', encoding='utf-8')\n",
    "\n",
    "# convert columns back to numeric as necessary\n",
    "\n",
    "all_trees_gdf['age'] = pd.to_numeric(all_trees_gdf[\"age\"])\n",
    "all_trees_gdf['circumference'] = pd.to_numeric(all_trees_gdf[\"circumference\"])\n",
    "all_trees_gdf['year'] = pd.to_numeric(all_trees_gdf[\"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the data, let's do some basic exploration\n",
    "\n",
    "Let's start by finding the oldest tree using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get oldest tree\n",
    "\n",
    "all_trees_gdf[all_trees_gdf['age']==all_trees_gdf['age'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a bit unlikely, particularly given it's height and circumference. At a guess, it was probably planted in 2017. Let's try to identify any other similar outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, that looks a bit spurious, particularly given it's height and circumference\n",
    "# At a guess, it was probably planted in 2017\n",
    "# Let's look at what other outliers there are\n",
    "\n",
    "all_trees_gdf.loc[(all_trees_gdf['age'] >= 1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This seems to show that anything with a year has a sensible age\n",
    "\n",
    "all_trees_gdf.loc[(all_trees_gdf['age'] == 0) & (all_trees_gdf['year'] >= 1) & (all_trees_gdf['year'] < 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but there are a lot of missing ages that have years\n",
    "\n",
    "all_trees_gdf.loc[(all_trees_gdf['age'].isnull()) & (all_trees_gdf['year'] >= 1) & (all_trees_gdf['year'] < 2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about circumference? \n",
    "\n",
    "all_trees_gdf.loc[(all_trees_gdf['circumference'] >= 500) & (all_trees_gdf['circumference'] <= 13000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give the oldest tree by  \n",
    "\n",
    "all_trees_gdf.sort_values('age').drop_duplicates(['borough'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give you the tree with the highest cirucmference for each borough \n",
    "\n",
    "# more columns can be added to the list passed to drop_duplicates to effectively group by more columns\n",
    "\n",
    "all_trees_gdf.sort_values('circumference').drop_duplicates(['borough'], keep='last').head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trees-api]",
   "language": "python",
   "name": "conda-env-trees-api-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
